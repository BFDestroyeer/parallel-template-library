#pragma once

#include <atomic>
#include <cassert>
#include <cstdint>
#include <thread>

#include "bitset"

namespace ptl
{
class mrlock
{
  protected:
    // static const uint32_t CACHELINE_SIZE = 128;

    struct cell
    {
        std::atomic<uint32_t> m_sequence;
        bitset m_bits;
        // char m_pad[CACHELINE_SIZE - sizeof(std::atomic<uint32_t>) - sizeof(uint64_t)];
    };

    // char m_pad0[CACHELINE_SIZE];
    cell *m_buffer;
    uint32_t m_buffer_mask;

    const uint32_t c_resource_size;

    // char m_pad1[CACHELINE_SIZE - sizeof(cell *) - sizeof(uint32_t)];
    std::atomic<uint32_t> m_head;

    // char m_pad[CACHELINE_SIZE - sizeof(std::atomic<uint32_t>)];
    std::atomic<uint32_t> m_tail;

  public:
    explicit mrlock(uint32_t resources) : c_resource_size(resources)
    {
        uint32_t max_threads = std::thread::hardware_concurrency();
        uint32_t buffer_size = 2;
        while (buffer_size <= max_threads)
        {
            buffer_size = buffer_size << 1;
        }
        assert((buffer_size >= 2) && ((buffer_size & (buffer_size - 1)) == 0));

        m_buffer = new cell[buffer_size];
        m_buffer_mask = buffer_size - 1;

        for (uint32_t i = 0; i < buffer_size; i++)
        {
            m_buffer[i].m_sequence.store(i, std::memory_order_relaxed);
            m_buffer[i].m_bits.resize(resources);

            m_buffer[i].m_bits = ~0;
        }

        m_head.store(0, std::memory_order_relaxed);
        m_tail.store(0, std::memory_order_relaxed);
    }

    constexpr ~mrlock()
    {
        delete[] m_buffer;
    }

    uint32_t random_access_lock(const size_t index)
    {
        bitset resource_mask;
        resource_mask.resize(this->c_resource_size);
        resource_mask.set(index);
        return this->lock(resource_mask);
    }

    uint32_t lock(const bitset &resources)
    {
        cell *cell;
        uint32_t pos;

        while (true)
        {
            pos = m_tail.load(std::memory_order_relaxed);
            cell = &m_buffer[pos & m_buffer_mask];
            uint32_t seq = cell->m_sequence.load(std::memory_order_acquire);
            int32_t dif = (int32_t)seq - (int32_t)pos;

            if (dif == 0)
            {
                if (m_tail.compare_exchange_weak(pos, pos + 1, std::memory_order_relaxed))
                {
                    break;
                }
            }
        }

        cell->m_bits = resources;
        cell->m_sequence.store(pos + 1, std::memory_order_release);

        uint32_t spin_pos = m_head;
        while (spin_pos != pos)
        {
            if (pos - m_buffer[spin_pos & m_buffer_mask].m_sequence > m_buffer_mask ||
                !(m_buffer[spin_pos & m_buffer_mask].m_bits & resources))
            {
                spin_pos++;
            }
        }
        return pos;
    }

    void unlock(uint32_t handle)
    {
        m_buffer[handle & m_buffer_mask].m_bits = 0;

        uint32_t pos = m_head.load(std::memory_order_relaxed);
        while (!m_buffer[pos & m_buffer_mask].m_bits)
        {
            cell *cell = &m_buffer[pos & m_buffer_mask];
            uint32_t seq = cell->m_sequence.load(std::memory_order_acquire);
            int32_t dif = (int32_t)seq - (int32_t)(pos + 1);

            if (dif == 0)
            {
                if (m_head.compare_exchange_weak(pos, pos + 1, std::memory_order_relaxed))
                {
                    cell->m_bits = ~0;
                    cell->m_sequence.store(pos + m_buffer_mask + 1, std::memory_order_release);
                }
            }

            pos = m_head.load(std::memory_order_relaxed);
        }
    }
};
} // namespace ptl